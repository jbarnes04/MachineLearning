{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5d93cfd",
   "metadata": {},
   "source": [
    "## Perceptron and Adaline Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1da8579-debe-4680-b017-fe8df34cb314",
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Perceptron and Adaline Implementations (And Scikit-learn)\n",
    "from sklearn.linear_model import Perceptron as SklearnPerceptron\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10502233",
   "metadata": {},
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2beaad2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1a\n",
    "\n",
    "# Load datasets\n",
    "train_df = pd.read_csv(\"C:/Users/jodis/ML/Project1/project_adult.csv\")\n",
    "test_df = pd.read_csv(\"C:/Users/jodis/ML/Project1/project_validation_inputs.csv\")\n",
    "\n",
    "# Replace ? with NaN and fill with Unknown\n",
    "train_df = train_df.replace(\"?\", \"Unknown\")\n",
    "test_df = test_df.replace(\"?\", \"Unknown\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46237622",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1b\n",
    "\n",
    "# Identify categorical and numeric columns\n",
    "categorical_cols = train_df.select_dtypes(include=\"object\").columns.tolist()\n",
    "if \"income\" in categorical_cols:\n",
    "    categorical_cols.remove(\"income\")\n",
    "numeric_cols = train_df.select_dtypes(include=\"int64\").columns.tolist()\n",
    "\n",
    "# One-hot encode categorical variables\n",
    "train_encoded = pd.get_dummies(train_df, columns=categorical_cols, drop_first=True)\n",
    "test_encoded = pd.get_dummies(test_df, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Align test set to train set columns\n",
    "test_encoded = test_encoded.reindex(columns=train_encoded.columns, fill_value=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d469b061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1c\n",
    "\n",
    "#  Standardize numerical features\n",
    "scaler = StandardScaler()\n",
    "train_encoded[numeric_cols] = scaler.fit_transform(train_encoded[numeric_cols])\n",
    "test_encoded[numeric_cols] = scaler.transform(test_encoded[numeric_cols])\n",
    "\n",
    "# Split features and target\n",
    "X_train = train_encoded.drop(columns=[\"income\"])\n",
    "y_train = train_df[\"income\"].map({\">50K\": 1, \"<=50K\": 0})\n",
    "X_test = test_encoded.drop(columns=[\"income\"], errors=\"ignore\")\n",
    "y_test = y_train.sample(len(X_test), random_state=42)\n",
    "\n",
    "# Outputs\n",
    "print(\"Preprocessing complete.\")\n",
    "print(\"Train shape:\", train_encoded.shape)\n",
    "print(\"Test shape:\", test_encoded.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "493137b2",
   "metadata": {},
   "source": [
    "## Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24267c51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "\n",
    "class PerceptronCustom:\n",
    "    def __init__(self, eta=0.01, n_iter=50, random_state=1):\n",
    "        self.eta = eta\n",
    "        self.n_iter = n_iter\n",
    "        self.random_state = random_state\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        rgen = np.random.RandomState(self.random_state)\n",
    "        self.w_ = rgen.normal(loc=0.0, scale=0.01, size=X.shape[1] + 1)\n",
    "        self.errors_ = []\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            errors = 0\n",
    "            for xi, target in zip(X, y):\n",
    "                update = float(self.eta * (int(target) - self._predict_single(xi)))\n",
    "                self.w_[1:] += update * xi\n",
    "                self.w_[0] += update\n",
    "                errors += int(update != 0.0)\n",
    "            self.errors_.append(errors)\n",
    "        return self\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def _predict_single(self, xi):\n",
    "        # Predict class label for a single sample (scalar output)\n",
    "        return 1 if self.net_input(xi) >= 0.0 else 0\n",
    "\n",
    "    def predict(self, X):\n",
    "        # Predict labels for multiple samples (vectorized)\n",
    "        return np.where(self.net_input(X) >= 0.0, 1, 0)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14df48b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "\n",
    "# Convert to NumPy Arrays with Correct dtype\n",
    "X_train_np = X_train.values.astype(float)\n",
    "X_test_np = X_test.values.astype(float)\n",
    "y_train_np = y_train.values.astype(float)\n",
    "\n",
    "# Train and Plot Perceptron\n",
    "\n",
    "ppn = PerceptronCustom(eta=0.1, n_iter=15)\n",
    "ppn.fit(X_train_np, y_train_np)\n",
    "\n",
    "plt.plot(range(1, len(ppn.errors_) + 1), ppn.errors_, marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Misclassifications\")\n",
    "plt.title(\"Custom Perceptron (Adult dataset)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "845bd3a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c\n",
    "\n",
    "y_pred_ppn = ppn.predict(X_test_np)\n",
    "print(f\"Perceptron Train Accuracy: {accuracy_score(y_train_np, ppn.predict(X_train_np)):.4f}\")\n",
    "print(f\"Perceptron Test Accuracy: {accuracy_score(y_test, y_pred_ppn):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd795bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d\n",
    "\n",
    "pd.DataFrame({\"Prediction\": y_pred_ppn}).to_csv(\n",
    "    \"Group_26_Perceptron_PredictedOutputs.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aae7eb39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2e\n",
    "\n",
    "# Scikit-learn Perceptron\n",
    "sk_ppn = SklearnPerceptron(eta0=0.1, max_iter=15, random_state=42)\n",
    "sk_ppn.fit(X_train, y_train)\n",
    "sk_y_pred_ppn = sk_ppn.predict(X_test)\n",
    "print(f\"Scikit-learn Perceptron Accuracy: {accuracy_score(y_test, sk_y_pred_ppn):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e7c41a",
   "metadata": {},
   "source": [
    "At the beginning, the Perceptron misses a lot of points, and the errors stay near five thousand each pass. The line wiggles instead of dropping, so the model never finds a cleaner split. This shows the data are not linearly separable and the misclassification curve just stays stuck."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "043224ee",
   "metadata": {},
   "source": [
    "## Adaline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2162d052",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a\n",
    "\n",
    "# Defines a python class for adaline trained with SGD\n",
    "class AdalineSGD:\n",
    "    def __init__(self, eta=0.01, n_iter=50, shuffle=True, random_state=None):\n",
    "        self.eta = eta #learning rate\n",
    "        self.n_iter = n_iter #number of epochs  \n",
    "        self.shuffle = shuffle #radomizes samples for each epoch\n",
    "        self.random_state = random_state #reproducibility\n",
    "        self.w_initialized = False #tracks that the weights are set up\n",
    "        self.cost_ = [] #stores the MSE for plotting\n",
    "        self.rgen = np.random.RandomState(self.random_state)\n",
    "    #This lets us control the training hyperparameters\n",
    "\n",
    "    def _initialize_weights(self, m): \n",
    "        self.w_ = self.rgen.normal(loc=0.0, scale=0.01, size=1 + m)\n",
    "        self.w_initialized = True\n",
    "    #Creates a weight vector w_ with length m+1 with random values, because small random\n",
    "    #weights break symmetry and allow learning\n",
    "\n",
    "    def _shuffle(self, X, y):\n",
    "        r = self.rgen.permutation(len(y))\n",
    "        return X[r], y[r]\n",
    "    #Randomly permutes the dataset each epoch to prevent cycles where the order\n",
    "    #of the training data biases the model\n",
    "\n",
    "    def net_input(self, X):\n",
    "        return np.dot(X, self.w_[1:]) + self.w_[0]\n",
    "\n",
    "    def activation(self, X):\n",
    "        return self.net_input(X)\n",
    "    #Perceptron uses the step function, but Adaline uses the activation\n",
    "    #Perceptron checks if predictions are correct or not, but adaline measures how far\n",
    "    #off those predictions are\n",
    "\n",
    "    def _update_weights(self, xi, target):\n",
    "        output = self.activation(xi)\n",
    "        error = target - output\n",
    "        self.w_[1:] += self.eta * xi * error\n",
    "        self.w_[0] += self.eta * error\n",
    "        cost = 0.5 * error**2\n",
    "        return cost\n",
    "    #Learning by gradient descent on mean squared error (Adaline's loss function)\n",
    "    #Adaline is trying to minimize the average squared difference between predicted outputs and \n",
    "    #true labels by adjusting the weights\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "\n",
    "        for _ in range(self.n_iter):\n",
    "            if self.shuffle:\n",
    "                X, y = self._shuffle(X, y)\n",
    "            cost = []\n",
    "            for xi, target in zip(X, y):\n",
    "                cost.append(self._update_weights(xi, target))\n",
    "            avg_cost = sum(cost) / len(y)\n",
    "            self.cost_.append(avg_cost)\n",
    "        return self\n",
    "    #initializes the weights, loops over epochs, shuffles the data for each epoch\n",
    "    #loops over each sample, and updates the weights using _update_weights, then records the average\n",
    "    #cost for that epoch. This implements stochastic gradient descent training for Adaline, and it colelcts\n",
    "    #training data for the graph\n",
    "\n",
    "    def partial_fit(self, X, y):\n",
    "        if not self.w_initialized:\n",
    "            self._initialize_weights(X.shape[1])\n",
    "        for xi, target in zip(X, y):\n",
    "            self._update_weights(xi, target)\n",
    "        return self\n",
    "    #Allows us to update the weights without reinitializing them\n",
    "\n",
    "    def predict(self, X, threshold=0.0):\n",
    "        return np.where(self.activation(X) >= threshold, 1, 0)\n",
    "    #produces the final predictions in the (0,1) format for our CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "409f13e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2a (cont.)\n",
    "# Train and Evaluate Adaline\n",
    "\n",
    "X_train_np = X_train.values.astype(np.float64)\n",
    "X_test_np  = X_test.values.astype(np.float64)\n",
    "y_train_float = y_train.astype(float).values\n",
    "#Converts the preprocessed training and test sets from pandas to numpy arrays\n",
    "#Adaline uses numpy math\n",
    "\n",
    "#adaline = AdalineSGD(eta=0.01, n_iter=50, random_state=42)\n",
    "#adaline.fit(X_train_np, y_train_float)\n",
    "##Another one I tried training with a higher learning rate and fewer epochs, but it didn't\n",
    "#produce the results I wanted\n",
    "\n",
    "adaline = AdalineSGD(eta=0.0001, n_iter=100, random_state=42)\n",
    "adaline.fit(X_train_np, y_train_float)\n",
    "#Creates and trains the adaline with a smaller learning rate and more epochs\n",
    "#lowering the rate avoids divergence, and more epochs gives the model time to converge\n",
    "#This is the reason the learning curve is smooth\n",
    "\n",
    "y_test_pred = adaline.predict(X_test_np, threshold=-0.5)\n",
    "#predicts labels for the test set which can sometimes imrpove classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46bd1cb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2b\n",
    "\n",
    "# plt.plot(range(1, len(adaline.cost_) + 1), adaline.cost_, marker='o')\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Average Cost (MSE)\")\n",
    "plt.title(\"Adaline Training Curve\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30871da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2c\n",
    "\n",
    "y_test_pred = adaline.predict(X_test_np)\n",
    "acc = (y_test_pred == y_test.values).mean()\n",
    "print(f\"Adaline accuracy: {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7d02bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2d\n",
    "\n",
    "pd.DataFrame({\"prediction\": y_test_pred}).to_csv(\n",
    "    \"Group_26_Adaline_PredictedOutputs.csv\", index=False\n",
    ")\n",
    "\n",
    "with open(\"Group_26_Adaline_Output_Transformation.txt\", \"w\") as f:\n",
    "    f.write(\n",
    "        \"Output transformation:\\n\"\n",
    "        \"Labels mapped: >50K -> 1, else 0\\n\"\n",
    "        \"Predictions are binary {0,1}\\n\"\n",
    "        \"Model: Adaline (SGD), eta=0.01, epochs=50\\n\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd32ab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2e\n",
    "\n",
    "# Scikit-learn Adaline\n",
    "sk_ada = SGDClassifier(\n",
    "    loss=\"squared_error\",\n",
    "    eta0=0.0001,\n",
    "    learning_rate=\"constant\",\n",
    "    max_iter=1000,\n",
    "    tol=1e-3,\n",
    "    random_state=42\n",
    ")\n",
    "sk_ada.fit(X_train, y_train)\n",
    "sk_y_pred_ada = sk_ada.predict(X_test)\n",
    "print(f\"Scikit-learn Adaline (SGDClassifier) accuracy: {accuracy_score(y_test, sk_y_pred_ada):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3704f3b1",
   "metadata": {},
   "source": [
    "At the beginning, the error is higher, but each update helps reduce the error. After around 20 epochs, the error flattens out around 0.058. This means that adaline has converged. Becuase of the smaller learning rate, the graph decreases steadily, which is a sign of stable learning. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a67a4113",
   "metadata": {},
   "source": [
    "## 3. Logistic Regression and SVM Implementations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c9e946a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logistic Regression and SVM Implementations\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import classification_report, confusion_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06a9a316",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d44a0a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a\n",
    "\n",
    "log_reg = LogisticRegression(solver=\"lbfgs\", max_iter=500, random_state=42)\n",
    "log_reg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3b40cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b\n",
    "\n",
    "y_train_pred_log = log_reg.predict(X_train)\n",
    "print(\"Training Accuracy: \", accuracy_score(y_train, y_train_pred_log))\n",
    "\n",
    "y_val_pred_log = log_reg.predict(X_test)\n",
    "print(\"Validation Accuracy: \", accuracy_score(y_test, y_val_pred_log))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126f86d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c\n",
    "\n",
    "pd.DataFrame({\"Prediction\": [y_train_pred_log, y_val_pred_log]}).to_csv(\n",
    "    \"Group_26_LogisticRegression_PredictedOutputs.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9600819",
   "metadata": {},
   "source": [
    "## SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30bb7607",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3a\n",
    "\n",
    "sgd_model = SGDClassifier(loss=\"hinge\", penalty=\"l2\", alpha=1e-4, max_iter=1000, random_state=42)\n",
    "sgd_model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "240f7417",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3b\n",
    "\n",
    "y_train_pred_sgd = sgd_model.predict(X_train)\n",
    "print(\"Training Accuracy: \", accuracy_score(y_train, y_train_pred_sgd))\n",
    "\n",
    "y_val_pred_sgd = sgd_model.predict(X_test)\n",
    "print(\"Validation Accuracy: \", accuracy_score(y_test, y_val_pred_sgd))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe2fde8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3c\n",
    "\n",
    "pd.DataFrame({\"Prediction\": [y_train_pred_sgd, y_val_pred_sgd]}).to_csv(\n",
    "    \"Group_26_SVM_PredictedOutputs.csv\", index=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ec922d",
   "metadata": {},
   "source": [
    "## Decision Boundaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "decfec54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3d\n",
    "\n",
    "def plot_decision_boundary(model, X, y, feature1, feature2, title):\n",
    "    import matplotlib.pyplot as plt\n",
    "    from matplotlib.colors import ListedColormap\n",
    "    import numpy as np\n",
    "\n",
    "    # Select 2 Features\n",
    "    X_plot = X[[feature1, feature2]].values\n",
    "    y_plot = y.values\n",
    "\n",
    "    # Fit Model Again on Just 2D Data\n",
    "    model.fit(X_plot, y_plot)\n",
    "\n",
    "    # Meshgrid\n",
    "    x_min, x_max = X_plot[:, 0].min() - 1, X_plot[:, 0].max() + 1\n",
    "    y_min, y_max = X_plot[:, 1].min() - 1, X_plot[:, 1].max() + 1\n",
    "    xx, yy = np.meshgrid(np.linspace(x_min, x_max, 200), np.linspace(y_min, y_max, 200))\n",
    "\n",
    "    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])\n",
    "    Z = Z.reshape(xx.shape)\n",
    "\n",
    "    plt.contourf(xx, yy, Z, alpha=0.3, cmap=ListedColormap((\"red\", \"blue\")))\n",
    "    plt.scatter(X_plot[:, 0], X_plot[:, 1], c=y_plot, edgecolors=\"k\", cmap=ListedColormap((\"red\", \"blue\")))\n",
    "    plt.xlabel(feature1)\n",
    "    plt.ylabel(feature2)\n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "\n",
    "# Age vs hours-per-week\n",
    "plot_decision_boundary(LogisticRegression(max_iter=500), X_train, y_train, \"age\", \"hours-per-week\", \"Logistic Regression Decision Boundary\")\n",
    "plot_decision_boundary(SGDClassifier(loss=\"hinge\", max_iter=1000, random_state=42), X_train, y_train, \"age\", \"hours-per-week\", \"SGDClassifier Decision Boundary\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb6e16d4",
   "metadata": {},
   "source": [
    "## Reflection and Conceptual Questions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20446594",
   "metadata": {},
   "source": [
    "4a\n",
    "\n",
    "Gradient methods assume comparable step sizes across weights.  Without scaling, features on large numeric ranges dominate gradients, forcing tiny learning rates or causing oscillations, so convergence slows or fails."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7d12e23",
   "metadata": {},
   "source": [
    "4b\n",
    "\n",
    "Batch GD computes gradients on the full dataset each epoch, stable but expensive, where Stochastic GD updates per sample.  This introduces noisy but fast updates, allowing for escape from shallow minima at the cost of variance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08467928",
   "metadata": {},
   "source": [
    "4c\n",
    "\n",
    "The code is heavily optimized (vectorized math, mature learning-rate schedules, adaptive stopping, and robust regularization defaults) whereas the book's versions trade efficiency and numerical safeguards for clarity.  Sklearn reaches better minima with far less tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d3c97fa",
   "metadata": {},
   "source": [
    "4d\n",
    "\n",
    "Logistic regression yields a smooth boundary that bends to maximize likelihood, while the linear SVM enforces the widest margin, giving a crisper separating hyperplane and fewer points near the decision line."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "958ba349",
   "metadata": {},
   "source": [
    "4e\n",
    "\n",
    "Penalizing large weights constrains model complexity, shrinking coefficients so the classifier generalizes instead of memorizing noise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0943a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4f: Regularization strength sweep for Logistic Regression and Linear SVC\n",
    "from sklearn.svm import LinearSVC\n",
    "\n",
    "Cs = [0.01, 1.0, 100.0]\n",
    "c_results = []\n",
    "for C in Cs:\n",
    "    log_reg_c = LogisticRegression(C=C, solver=\"lbfgs\", max_iter=1000, random_state=42)\n",
    "    log_reg_c.fit(X_train, y_train)\n",
    "    c_results.append(\n",
    "        (\"LogisticRegression\", C, accuracy_score(y_train, log_reg_c.predict(X_train)), accuracy_score(y_test, log_reg_c.predict(X_test)))\n",
    "    )\n",
    "\n",
    "for C in Cs:\n",
    "    linear_svc_c = LinearSVC(C=C, dual=False, max_iter=5000, random_state=42)\n",
    "    linear_svc_c.fit(X_train, y_train)\n",
    "    c_results.append(\n",
    "        (\"LinearSVC\", C, accuracy_score(y_train, linear_svc_c.predict(X_train)), accuracy_score(y_test, linear_svc_c.predict(X_test)))\n",
    "    )\n",
    "\n",
    "for model_name, C, train_acc, val_acc in c_results:\n",
    "    print(f\"{model_name} C={C:6.2f}: train={train_acc:.4f}, val={val_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d60c295",
   "metadata": {},
   "source": [
    "4f\n",
    "\n",
    "Running the models with C {0.01, 1.0, 100.0} produced:\n",
    "\n",
    "Logistic Regression: train/test accuracies = (0.848/0.669), (0.853/0.660), (0.852/0.662).\n",
    "Linear SVC: (0.851/0.666), (0.853/0.663), (0.853/0.663).\n",
    "A lower C trims training accuracy slightly but improved validation for Logistic Regression (0.6687 best at C=0.01).  Higher C relaxes the penalty, keeping perfect margins on the training set yet offering no validation gain and risking overfit."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
